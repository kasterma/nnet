---
title: "Notes Lecture 7: Recurrent Neural Networks"
output: html_document
---

# Modeling Sequences; a brief overview

Use methods from supervised learning, but doesn't require a separate teaching
signal.  (next point in sequence, pixels or patch in picture from whole
picture)

Autoregressive models; predict a term from a fixed length of previous terms
in the sequence.  We can do this just as an average of these previous terms,
or give the prediction some hidden state.  This hidden state can then have
its own internal dynamics; i.e. we can move from memoryless (e.g. weighted
average of previous terms) to a model than can store information in its hidden
state for a long time.

If the dynamics of the hidden state are noisy, and the way it generates
observations from the hidden state is noisy, we can never know the hidden
state exactly.

There are two situations in which the deduction of the state is tractable.

- linear dynamical system: the hidden state has linear dynamics with Gaussian
  noise, and the observations use a linear model with Gaussian noise.  Here we
  can compute the distribution of the hidden state using Kalman filters.
- hidden markov model; can use dynamic programming for getting the distribution
  of the hidden states, and then have an efficient learning model.

An HMM with n hidden states has only memory for log(n) bits.  For generating
e.g. speech this is a clear restriction (can only remember a limited amount
about first part of sentence when generating the second half; intonation,
syntax, semantics, accent of speaker, ...).

Recurrent neural networks:

- distributed hidden state; this allows them to store lots of information
  about the past efficiently.
- nonlinear dynamics; this allows them to update the hidden state in complicated
  ways.

Recurrent neural networks are deterministic in contrast to the linear systems
and HMM mentioned above.  But from the observations so far we have a
deterministic method for estimating the distribution of the hidden state.  The
deterministic state of the RNN can be compared to those estimates.

RNN can show lots of different behaviors, but because of this computational
power they are hard to train.

# Training RNNs with backpropagation

Assume: integer time, where every link is evaluated at every tick.

Equivalence between layered feedforward network with weight constraints and RNN
by unrolling the network.

Note: it is easy to modify the backpropagation algorithm to incorporate linear
restraints between the weights.

- compute the gradients as usual
- modify the gradients so they maintain the contraints.

To contrain: $w_1 = w_2$, need to ensure $\Delta w_1 = \Delta w_2$.  Compute
$\delta E / \delta w_i$, and use $\delta E / \delta w_0 + \delta E / \delta w_1$
for both $w_1$ and $w_2$.

(maybe better average; mentioned in the video, but not explained when to choose,
but if you average the total change is still of the same size order as the other
changes in stead of being twice too big.  Does this matter?)

There is an extra issue to deal with; we need to give an initial state to all
hidden units.  Can learn these parameters like you learn the rest; give
initial guesses, and then learn them during backprop.

# A toy example of training an RNN

Learn binary addition, where at each timestep get the next digits.  Can learn
this with an FSA, with four states.  Note, one step get input, next step use
it in its output; output is delayed one step past first guess.

Can do this with a network that has three hidden states, fully connected.  These
states learn four different activity patterns that correspond to the states of
the FSA.  So have correspondence, state in FSA, activity vector in RNN.  In this
way an RNN with n nodes has $2^n$ binary activation vectors (though states can
be more subtle than that).  


# What makes it difficult to train an RNN

Exploding and vanishing gradient problem.  These effects here are greater b/c
with the number of steps being larger than number of hidden layers in a deep
network it happens.  Can be partly dealt with by choosing good initial weigths.

Example with two color regions

Four effective ways to learn an RNN

- LSTM (change architecture to make it good at learning things; make it out of
  little modules)
- Hessian Free Optimization (use different optimizer that can deal with very
  small gradients)
- Echo State Networks (very carefully chosen hidden network, where then mostly
  just the hidden -> output is learned)
- Good initialization with momentum
